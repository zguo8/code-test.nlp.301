{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "Fji9_HZsR0FE",
    "outputId": "50229b62-ee64-4c5e-f777-0f61c77f64a3"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import datetime\n",
    "import traceback\n",
    "import xml.etree.ElementTree as ET  \n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import classification_report,roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pickle\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "\n",
    "def print_time(*args):\n",
    "    text = \"\"\n",
    "    for arg in args:\n",
    "        text += arg\n",
    "    print(\"[\",datetime.datetime.now(),\"]\", text)\n",
    "    \n",
    "\n",
    "def pickle_save(filename, data2pkl):\n",
    "    global pickle_overwrite\n",
    "    if pickle_overwrite == True:\n",
    "        fileObject = open(filename,'wb') \n",
    "        pickle.dump(data2pkl,fileObject)\n",
    "        fileObject.close()\n",
    "\n",
    "        \n",
    "def pickle_load(filename):\n",
    "    fileObject = open(filename,'rb') \n",
    "    data_unpkl = pickle.load(fileObject)\n",
    "    fileObject.close()\n",
    "    return data_unpkl\n",
    "\n",
    "\n",
    "# Logistic Regression model\n",
    "def run_lr(dataset, label_set):\n",
    "    split_num = int(len(dataset)*0.9)\n",
    "    print(\"split num\", split_num)\n",
    "    train_set = dataset[:split_num]\n",
    "    test_set = dataset[split_num:]\n",
    "    train_label = label_set[:split_num]\n",
    "    test_label = label_set[split_num:]\n",
    "    \n",
    "    print(\"training set\", len(train_set),len(train_label),\"testing set\", len(test_set),len(test_label))\n",
    "    \n",
    "    print_time(\"Start LR...\")\n",
    "\n",
    "    lr = LogisticRegression(max_iter = 10) #, class_weight = {\"0\":.83, \"1\":.17}\n",
    "\n",
    "    lr.fit(train_set, train_label)\n",
    "    print_time(\"Complete LR...\")\n",
    "    \n",
    "    lr_result = lr.predict(test_set)\n",
    "    lr_probs = lr.predict_proba(test_set)\n",
    "    \n",
    "    lr_probs_trans = list(map(float,np.hstack(lr_probs[:,1])))\n",
    "\n",
    "    print(classification_report(test_label, lr_result))\n",
    "    print('ROC-AUC: ', roc_auc_score(test_label, lr_probs_trans))\n",
    "\n",
    "    print(sum(list(map(int, lr_result))))\n",
    "    \n",
    "    \n",
    "# display the keywords of each LDA topic    \n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        word_idx = np.argsort(topic)[::-1][:no_top_words]\n",
    "        \n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# plot word cloud\n",
    "def show_wordcloud(corpus):\n",
    "    stopwords = set(STOPWORDS)\n",
    "\n",
    "    # Create and generate a word cloud image:\n",
    "    wordcloud = WordCloud(stopwords=stopwords).generate(corpus)\n",
    "\n",
    "    # Display the generated image:\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "xml_file = \"data/AviationData.xml\"\n",
    "\n",
    "narrative_path  = \"data/*.json\"\n",
    "\n",
    "out_csv = \"output/aviation_accidents.csv\"\n",
    "\n",
    "total_event_xml = 0\n",
    "\n",
    "event_record_dict = {} # key: event, value: dict of attributes\n",
    "\n",
    "# decide if need to pickle data    \n",
    "pickle_overwrite = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rS7-dmDgeFyk"
   },
   "outputs": [],
   "source": [
    "# process xml file\n",
    "try:\n",
    "    tree = ET.parse(xml_file)  \n",
    "    root = tree.getroot()\n",
    "    \n",
    "    attr_dict = root[0][0].attrib\n",
    "    \n",
    "    for item in root:\n",
    "            for subitem in item:\n",
    "                event_json = subitem.attrib\n",
    "                if total_event_xml == 1:                    \n",
    "                    print(subitem.attrib)\n",
    "                total_event_xml += 1\n",
    "                event_record_dict[event_json.get(\"EventId\")] = event_json\n",
    "        \n",
    "        \n",
    "    with open(out_csv, mode='w') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(attr_dict.keys())\n",
    "\n",
    "        for item in root:\n",
    "            for subitem in item:\n",
    "                if total_event_xml == 1:\n",
    "                    print(subitem.attrib)\n",
    "                writer.writerow(subitem.attrib.values())\n",
    "                total_event_xml += 1\n",
    "        \n",
    "except Exception as ex:\n",
    "    sys.stderr.write('Exception\\n')\n",
    "    extype, exvalue, extrace = sys.exc_info()\n",
    "    traceback.print_exception(extype, exvalue, extrace)     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKAdc0O1U95o"
   },
   "outputs": [],
   "source": [
    "# process json files\n",
    "event_nar_dict = {}\n",
    "\n",
    "narrative_files = glob.glob(narrative_path)\n",
    "total_event_json = 0\n",
    "\n",
    "nar_corpus = []\n",
    "cause_corpus = []\n",
    "combined_corpus = []\n",
    "\n",
    "for name in narrative_files:\n",
    "    try:\n",
    "        with open(name) as file:\n",
    "            data = json.load(file).get(\"data\")\n",
    "            for event in data:\n",
    "                event_id = event.get(\"EventId\")\n",
    "                \n",
    "                event_nar_dict[event_id] = event                \n",
    "\n",
    "                nar_corpus.append(event.get(\"narrative\"))\n",
    "                cause_corpus.append(event.get(\"probable_cause\"))\n",
    "                combined_corpus.append(event.get(\"narrative\") + event.get(\"probable_cause\"))\n",
    "                \n",
    "                if event_record_dict.get(event_id) != None:\n",
    "                    record_dict = event_record_dict.get(event_id)\n",
    "                    record_dict[\"narrative\"] = event.get(\"narrative\")\n",
    "                    record_dict[\"probable_cause\"] = event.get(\"probable_cause\")\n",
    "                    \n",
    "                    svrt = record_dict.get(\"InjurySeverity\")\n",
    "                    if svrt[:5].lower() == \"fatal\":\n",
    "                        record_dict[\"fatalness\"] =\"1\"\n",
    "                    else:\n",
    "                        record_dict[\"fatalness\"] =\"0\"\n",
    "                else:\n",
    "                    print(\"[ERR] Didn't find EventId in xml data.\")\n",
    "        \n",
    "            total_event_json += len(data)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        sys.stderr.write('Exception\\n')\n",
    "        extype, exvalue, extrace = sys.exc_info()\n",
    "        traceback.print_exception(extype, exvalue, extrace)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "f8b6lN-7daUJ",
    "outputId": "84805cc2-3519-4c78-9f40-d13564921610"
   },
   "outputs": [],
   "source": [
    "# preprocessing, filter out data without probable_cause for word cloud\n",
    "data_set = []\n",
    "label_set = []\n",
    "\n",
    "fatal_corpus = []\n",
    "nonfatal_corpus = []\n",
    "\n",
    "for key, value in event_record_dict.items():\n",
    "    cause = value.get(\"probable_cause\")\n",
    "    \n",
    "    # preprocessing of the text, text = cause + narrative\n",
    "    text = value.get(\"narrative\") + value.get(\"probable_cause\")\n",
    "\n",
    "    if text[:4].upper() == \"NTSB\" or text[:21].lower() == \"the foreign authority\":\n",
    "        text = text.split(\".\",1)[1].lower()\n",
    "        text = text.replace(\"plt\", \"pilot\").replace(\"flt\", \"flight\")\n",
    "        \n",
    "    label = value.get(\"fatalness\")\n",
    "    \n",
    "    if text != \"\":\n",
    "        data_set.append(text)\n",
    "        label_set.append(label)\n",
    "        \n",
    "    if cause != \"\":\n",
    "        if label == \"1\":\n",
    "            fatal_corpus.append(cause)\n",
    "        else:\n",
    "            nonfatal_corpus.append(cause)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "colab_type": "code",
    "id": "yIj73bcofzE0",
    "outputId": "5e8eee47-94dd-4163-ac19-850a433d0b74"
   },
   "outputs": [],
   "source": [
    "# show word cloud for fatal/nonfatal events    \n",
    "fatal_corpus_str = ' '.join(fatal_corpus)\n",
    "show_wordcloud(fatal_corpus_str)    \n",
    "\n",
    "nonfatal_corpus_str = ' '.join(nonfatal_corpus)\n",
    "show_wordcloud(nonfatal_corpus_str)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wPlhwKalGbgQ",
    "outputId": "658a7de1-f877-47a4-8f2f-18695ac2c302"
   },
   "outputs": [],
   "source": [
    "# topic modeling with sklearn LDA\n",
    "\n",
    "no_features = 200\n",
    "no_top_words = 10\n",
    "no_topics = 10\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "data_set_stem = []\n",
    "\n",
    "for item in data_set:    \n",
    "    data_set_stem.append(porter.stem(item))\n",
    "\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, ngram_range=(1, 2),token_pattern=r'\\b\\w+\\b', max_features=no_features, stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_set_stem)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "print_time(\"Complete TF-IDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "C5EMvMZC9JdD",
    "outputId": "437f6f26-599c-434f-a271-e5359c7a0b46"
   },
   "outputs": [],
   "source": [
    "print_time(\"Start LDA...\")\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "\n",
    "lda_learner = lda.fit_transform(tf)\n",
    "\n",
    "print_time(\"Complete LDA.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "d29HtHH-9eCW",
    "outputId": "ab9bf747-fbbf-4895-ea6d-567d601f425c"
   },
   "outputs": [],
   "source": [
    "# in case model run for too long, pickle data\n",
    "# pickle_save(\"lda_learner_75590.pkl\", lda_learner)\n",
    "# pickle_save(\"lda_75590.pkl\", lda)\n",
    "\n",
    "# lda_learner = pickle_load(\"lda_learner_75590.pkl\")\n",
    "# lad = pickle_load(\"lda_75590.pkl\")\n",
    "\n",
    "\n",
    "# display_topics(lda, tf_feature_names, no_top_words)\n",
    "\n",
    "x = {}\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx))\n",
    "    topic_keywords = \" \".join([tf_feature_names[i]\n",
    "                    for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "    print(topic_keywords)\n",
    "    topic_dict[topic_idx] = topic_keywords\n",
    "\n",
    "topic_fatal = [0] * 10\n",
    "topic_nonfatal = [0] * 10\n",
    "\n",
    "for i,item in enumerate(lda_learner):   \n",
    "    item = item.tolist()\n",
    "    topic = item.index(max(item)) \n",
    "    if label_set[i] == \"1\":\n",
    "        topic_fatal[topic] = topic_fatal[topic] + 1\n",
    "    else:\n",
    "        topic_nonfatal[topic] = topic_nonfatal[topic] + 1\n",
    "\n",
    "for i in range(10):\n",
    "    print(topic_fatal[i]/topic_nonfatal[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "hgTgPhox-jRP",
    "outputId": "a71045e3-5051-466c-9fb8-0c845050213e"
   },
   "outputs": [],
   "source": [
    "# run Logistic Regression and show metrics report\n",
    "run_lr(lda_learner, label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "F8GM1HJN9B2W",
    "outputId": "13f8cf25-5103-412d-d994-902e124de778"
   },
   "outputs": [],
   "source": [
    "# plot a bar chart to show the fatal/nonfatal event counts in each topic\n",
    "N = len(topic_dict)\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.3       # the width of the bars\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "rects1 = ax.bar(ind, topic_fatal, width, color='r')\n",
    "\n",
    "rects2 = ax.bar(ind+width, topic_nonfatal, width, color='g')\n",
    "\n",
    "x_labels = []\n",
    "for value in topic_dict.values():\n",
    "    x_labels.append(value.replace(\" s\",\"\").replace(\" \", \"\\n\"))\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.legend( (rects1[0], rects2[0]), ('fatal', 'nonfatal') )\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "aviation_analysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
